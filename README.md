# AgentEval_understanding
Agent Evaluation 

# AgentEval Framework Overview

AgentEval is a comprehensive framework for evaluating LLM-powered applications, consisting of three main agents:

1. CriticAgent: Defines evaluation criteria
2. QuantifierAgent: Quantifies performance against criteria
3. VerifierAgent: Ensures robustness and relevance of criteria

Sample Implementation
Here's a practical example of using AgentEval for a math problem-solving application:


# Reference:

https://github.com/Narabzad/AgentEval/tree/main

https://github.com/ag2ai/ag2/tree/main/autogen/agentchat/contrib/agent_eval

https://microsoft.github.io/autogen/0.2/blog/2024/06/21/AgentEval/

https://microsoft.github.io/autogen/0.2/blog/2023/11/20/AgentEval/

https://github.com/microsoft/autogen/blob/0.2/notebook/agenteval_cq_math.ipynb

https://github.com/ag2ai/ag2/blob/main/test/test_files/agenteval-in-out/samples/sample_math_evaluated_results.json


# Langsmith
1. https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/agent_steps/evaluating_agents.ipynb
